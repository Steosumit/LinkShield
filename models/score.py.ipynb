{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# A Simple URL Classifer",
   "id": "f5ca5d41428c782e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following notebook contains code to build a simple URL classifier from training to testing it with both CPU and GPU support models. Just dump the dataset, tweak the code and run, learn on the go!\n",
    "\n",
    "Made by: Sumit Gupta (steosumit)"
   ],
   "id": "e7302d82f545034d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import the libraries",
   "id": "35f2769cf1d52223"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T19:26:19.423842Z",
     "start_time": "2025-11-06T19:26:19.417874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "#from oracle_automl import AutoML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ],
   "id": "4ead0bd04ff64262",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Download and load the dataset",
   "id": "a793c7426c30a453"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T19:26:21.879362Z",
     "start_time": "2025-11-06T19:26:21.604387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PATH = \"D:\\Work\\Projects\\LinkShield\\data\\dataset_phishing.csv\"  # Change this accordingly\n",
    "df = pd.read_csv(PATH)\n",
    "df.describe()"
   ],
   "id": "e4cb506bc22bf022",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         length_url  length_hostname            ip       nb_dots  \\\n",
       "count  11430.000000     11430.000000  11430.000000  11430.000000   \n",
       "mean      61.126684        21.090289      0.150569      2.480752   \n",
       "std       55.297318        10.777171      0.357644      1.369686   \n",
       "min       12.000000         4.000000      0.000000      1.000000   \n",
       "25%       33.000000        15.000000      0.000000      2.000000   \n",
       "50%       47.000000        19.000000      0.000000      2.000000   \n",
       "75%       71.000000        24.000000      0.000000      3.000000   \n",
       "max     1641.000000       214.000000      1.000000     24.000000   \n",
       "\n",
       "         nb_hyphens         nb_at         nb_qm        nb_and    nb_or  \\\n",
       "count  11430.000000  11430.000000  11430.000000  11430.000000  11430.0   \n",
       "mean       0.997550      0.022222      0.141207      0.162292      0.0   \n",
       "std        2.087087      0.155500      0.364456      0.821337      0.0   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.0   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.0   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.0   \n",
       "75%        1.000000      0.000000      0.000000      0.000000      0.0   \n",
       "max       43.000000      4.000000      3.000000     19.000000      0.0   \n",
       "\n",
       "              nb_eq  ...   empty_title  domain_in_title  \\\n",
       "count  11430.000000  ...  11430.000000     11430.000000   \n",
       "mean       0.293176  ...      0.124759         0.775853   \n",
       "std        0.998317  ...      0.330460         0.417038   \n",
       "min        0.000000  ...      0.000000         0.000000   \n",
       "25%        0.000000  ...      0.000000         1.000000   \n",
       "50%        0.000000  ...      0.000000         1.000000   \n",
       "75%        0.000000  ...      0.000000         1.000000   \n",
       "max       19.000000  ...      1.000000         1.000000   \n",
       "\n",
       "       domain_with_copyright  whois_registered_domain  \\\n",
       "count           11430.000000             11430.000000   \n",
       "mean                0.439545                 0.072878   \n",
       "std                 0.496353                 0.259948   \n",
       "min                 0.000000                 0.000000   \n",
       "25%                 0.000000                 0.000000   \n",
       "50%                 0.000000                 0.000000   \n",
       "75%                 1.000000                 0.000000   \n",
       "max                 1.000000                 1.000000   \n",
       "\n",
       "       domain_registration_length    domain_age   web_traffic    dns_record  \\\n",
       "count                11430.000000  11430.000000  1.143000e+04  11430.000000   \n",
       "mean                   492.532196   4062.543745  8.567566e+05      0.020122   \n",
       "std                    814.769415   3107.784600  1.995606e+06      0.140425   \n",
       "min                     -1.000000    -12.000000  0.000000e+00      0.000000   \n",
       "25%                     84.000000    972.250000  0.000000e+00      0.000000   \n",
       "50%                    242.000000   3993.000000  1.651000e+03      0.000000   \n",
       "75%                    449.000000   7026.750000  3.738455e+05      0.000000   \n",
       "max                  29829.000000  12874.000000  1.076799e+07      1.000000   \n",
       "\n",
       "       google_index     page_rank  \n",
       "count  11430.000000  11430.000000  \n",
       "mean       0.533946      3.185739  \n",
       "std        0.498868      2.536955  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.000000      1.000000  \n",
       "50%        1.000000      3.000000  \n",
       "75%        1.000000      5.000000  \n",
       "max        1.000000     10.000000  \n",
       "\n",
       "[8 rows x 87 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_url</th>\n",
       "      <th>length_hostname</th>\n",
       "      <th>ip</th>\n",
       "      <th>nb_dots</th>\n",
       "      <th>nb_hyphens</th>\n",
       "      <th>nb_at</th>\n",
       "      <th>nb_qm</th>\n",
       "      <th>nb_and</th>\n",
       "      <th>nb_or</th>\n",
       "      <th>nb_eq</th>\n",
       "      <th>...</th>\n",
       "      <th>empty_title</th>\n",
       "      <th>domain_in_title</th>\n",
       "      <th>domain_with_copyright</th>\n",
       "      <th>whois_registered_domain</th>\n",
       "      <th>domain_registration_length</th>\n",
       "      <th>domain_age</th>\n",
       "      <th>web_traffic</th>\n",
       "      <th>dns_record</th>\n",
       "      <th>google_index</th>\n",
       "      <th>page_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11430.000000</td>\n",
       "      <td>11430.000000</td>\n",
       "      <td>11430.000000</td>\n",
       "      <td>11430.000000</td>\n",
       "      <td>11430.000000</td>\n",
       "      <td>11430.000000</td>\n",
       "      <td>11430.000000</td>\n",
       "      <td>11430.000000</td>\n",
       "      <td>11430.0</td>\n",
       "      <td>11430.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11430.000000</td>\n",
       "      <td>11430.000000</td>\n",
       "      <td>11430.000000</td>\n",
       "      <td>11430.000000</td>\n",
       "      <td>11430.000000</td>\n",
       "      <td>11430.000000</td>\n",
       "      <td>1.143000e+04</td>\n",
       "      <td>11430.000000</td>\n",
       "      <td>11430.000000</td>\n",
       "      <td>11430.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>61.126684</td>\n",
       "      <td>21.090289</td>\n",
       "      <td>0.150569</td>\n",
       "      <td>2.480752</td>\n",
       "      <td>0.997550</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.141207</td>\n",
       "      <td>0.162292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124759</td>\n",
       "      <td>0.775853</td>\n",
       "      <td>0.439545</td>\n",
       "      <td>0.072878</td>\n",
       "      <td>492.532196</td>\n",
       "      <td>4062.543745</td>\n",
       "      <td>8.567566e+05</td>\n",
       "      <td>0.020122</td>\n",
       "      <td>0.533946</td>\n",
       "      <td>3.185739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>55.297318</td>\n",
       "      <td>10.777171</td>\n",
       "      <td>0.357644</td>\n",
       "      <td>1.369686</td>\n",
       "      <td>2.087087</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.364456</td>\n",
       "      <td>0.821337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330460</td>\n",
       "      <td>0.417038</td>\n",
       "      <td>0.496353</td>\n",
       "      <td>0.259948</td>\n",
       "      <td>814.769415</td>\n",
       "      <td>3107.784600</td>\n",
       "      <td>1.995606e+06</td>\n",
       "      <td>0.140425</td>\n",
       "      <td>0.498868</td>\n",
       "      <td>2.536955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-12.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>972.250000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>242.000000</td>\n",
       "      <td>3993.000000</td>\n",
       "      <td>1.651000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>71.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>449.000000</td>\n",
       "      <td>7026.750000</td>\n",
       "      <td>3.738455e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1641.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>29829.000000</td>\n",
       "      <td>12874.000000</td>\n",
       "      <td>1.076799e+07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 87 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "8cb595380aaafa98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T19:26:24.503926Z",
     "start_time": "2025-11-06T19:26:24.481997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select the features\n",
    "features = [\n",
    "    'length_url', 'length_hostname', 'ip', 'nb_dots', 'nb_hyphens', 'nb_at', 'nb_qm', 'nb_and', 'nb_or', 'nb_eq',\n",
    "    'nb_underscore', 'nb_tilde', 'nb_percent', 'nb_slash', 'nb_star', 'nb_colon', 'nb_comma', 'nb_semicolumn',\n",
    "    'nb_dollar', 'nb_space', 'nb_www', 'nb_com', 'nb_dslash', 'http_in_path', 'https_token', 'ratio_digits_url',\n",
    "    'ratio_digits_host', 'punycode', 'shortening_service', 'path_extension', 'phish_hints', 'domain_in_brand',\n",
    "    'brand_in_subdomain', 'brand_in_path', 'suspecious_tld'\n",
    "]\n",
    "numerical_df = df.select_dtypes('float64', 'int64')\n",
    "corr_matrix = numerical_df.corr()\n",
    "for s in corr_matrix:\n",
    "    print(s)"
   ],
   "id": "7cefad2ea8a25bc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio_digits_url\n",
      "ratio_digits_host\n",
      "avg_words_raw\n",
      "avg_word_host\n",
      "avg_word_path\n",
      "ratio_intHyperlinks\n",
      "ratio_extHyperlinks\n",
      "ratio_extRedirection\n",
      "ratio_extErrors\n",
      "links_in_tags\n",
      "ratio_intMedia\n",
      "ratio_extMedia\n",
      "safe_anchor\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Engineering : Correlation based feature selection",
   "id": "3ddf32d2d465f28f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T19:26:26.601393Z",
     "start_time": "2025-11-06T19:26:26.593277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"Developer: Do not iterate of the corr_matrix directly, do it column wise as or states corr scores of one column with others\"\"\"\n",
    "def feature_selector_correlation(cmatrix, target_cols, threshold=0.1):\n",
    "    selected_features = {}\n",
    "    for target_col in target_cols:\n",
    "        correlations = cmatrix[target_col]  # correlation of all features with the target\n",
    "        selected_features = {}\n",
    "        for feature, score in correlations.items():\n",
    "            if abs(score) >= threshold and feature != target_col:\n",
    "                selected_features[feature] = score\n",
    "    return selected_features\n",
    "selected_features = list((feature_selector_correlation(corr_matrix, numerical_df.columns, threshold=0.01)).keys())"
   ],
   "id": "9adbfc254340193d",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T19:33:25.980326Z",
     "start_time": "2025-11-06T19:33:25.927158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training code\n",
    "## Splitting the dataset\n",
    "X = df[selected_features]\n",
    "y = df['status']\n",
    "\n",
    "# Encode the target variable (convert 'legitimate'/'phishing' to 0/1)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.25, random_state=42)\n",
    "\n",
    "## Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\"\"\"\n",
    "My code ended here, the rest is a AI assisted coding content, so please so have extra watch\n",
    "\"\"\"\n"
   ],
   "id": "a87d342ecbf7eccb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels: ['legitimate' 'phishing']\n",
      "Encoded labels: ['legitimate' 'phishing']\n",
      "Mapping: {'legitimate': np.int64(0), 'phishing': np.int64(1)}\n",
      "\n",
      "✓ Label encoder and scaler saved!\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training the model using AutoML (Oracle Propreitary Library)",
   "id": "e0c21e8d88a8c756"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will be using the Oracle AutoML library to train the simple classifier that runs various model training algorithms and state the best performing models",
   "id": "1d6dd569ce53c31f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### AutoMLx: Making model training function by automlx",
   "id": "39795a3d9c1f5bda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T14:33:28.515417Z",
     "start_time": "2025-11-06T14:33:28.483377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from automlx import init, Pipeline\n",
    "def train_phishing_classifier(X: pd.DataFrame,\n",
    "                              y: pd.Series,\n",
    "                              test_size: float = 0.2,\n",
    "                              random_state: int = 42,\n",
    "                              task: str = 'classification',\n",
    "                              **automlx_kwargs):\n",
    "    \"\"\"\n",
    "    Train a classifier using Oracle AutoMLx on the supplied DataFrame.\n",
    "    Returns the trained pipeline/model and test performance report.\n",
    "    \"\"\"\n",
    "    # 3. Initialize AutoMLx engine (optional customization)\n",
    "    init(engine='local')  # you may pass n_jobs, etc.\n",
    "    # 4. Create AutoMLx pipeline\n",
    "    pipeline = Pipeline(task=task, random_state=random_state, **automlx_kwargs)\n",
    "    # 5. Fit the pipeline\n",
    "    pipeline.fit(X, y)\n",
    "    # 6. Make predictions on test set\n",
    "    y_pred = pipeline.predict(X_test_scaled)\n",
    "    # 7. Evaluate\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    return pipeline, report\n",
    "\n",
    "train_phishing_classifier(X_train_scaled, y_train, test_size=0.25, random_state=42, task='classification')\n"
   ],
   "id": "d44fd748534469e9",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'automlx'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mautomlx\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m init, Pipeline\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtrain_phishing_classifier\u001B[39m(X: pd.DataFrame,\n\u001B[32m      3\u001B[39m                               y: pd.Series,\n\u001B[32m      4\u001B[39m                               test_size: \u001B[38;5;28mfloat\u001B[39m = \u001B[32m0.2\u001B[39m,\n\u001B[32m      5\u001B[39m                               random_state: \u001B[38;5;28mint\u001B[39m = \u001B[32m42\u001B[39m,\n\u001B[32m      6\u001B[39m                               task: \u001B[38;5;28mstr\u001B[39m = \u001B[33m'\u001B[39m\u001B[33mclassification\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m      7\u001B[39m                               **automlx_kwargs):\n\u001B[32m      8\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[33;03m    Train a classifier using Oracle AutoMLx on the supplied DataFrame.\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[33;03m    Returns the trained pipeline/model and test performance report.\u001B[39;00m\n\u001B[32m     11\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'automlx'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Manual Approach: making the model training function by manually specifying the model",
   "id": "7469dd3945fad9e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adding CPU models",
   "id": "12b044d93b2301a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T19:34:37.660133Z",
     "start_time": "2025-11-06T19:34:18.374547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "\n",
    "USE_GPU = True  # Change to False to use CPU only\n",
    "\n",
    "# Step 1: Make a model type list, adding models manually based on the situation( both GPU and CPU)\n",
    "model_type_list = {}\n",
    "\n",
    "# Add sklearn models (CPU only)\n",
    "model_type_list.update({\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'GaussianNB': GaussianNB()\n",
    "})\n",
    "\n",
    "print(f\"\\nGPU + CPU Total models: {len(model_type_list)}\")\n",
    "print(f\"Models: {list(model_type_list.keys())}\\n\")\n"
   ],
   "id": "fd5aa23f02c40aa5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ XGBoost with GPU enabled\n",
      "✓ LightGBM with GPU enabled\n",
      "\n",
      "✓ Total models: 10\n",
      "Models: ['XGBoost', 'LightGBM', 'LogisticRegression', 'DecisionTree', 'RandomForest', 'GradientBoosting', 'AdaBoost', 'SVM', 'KNN', 'GaussianNB']\n",
      "\n",
      "Training: XGBoost\n",
      "Training with default parameters...\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy:  0.8555\n",
      "  Precision: 0.8556\n",
      "  Recall:    0.8555\n",
      "  F1-Score:  0.8555\n",
      "Training: LightGBM\n",
      "Training with default parameters...\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy:  0.8481\n",
      "  Precision: 0.8482\n",
      "  Recall:    0.8481\n",
      "  F1-Score:  0.8481\n",
      "Training: LogisticRegression\n",
      "Performing Grid Search with 3-fold CV...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters: {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best CV score: 0.6893\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy:  0.7043\n",
      "  Precision: 0.7105\n",
      "  Recall:    0.7043\n",
      "  F1-Score:  0.7024\n",
      "Training: DecisionTree\n",
      "Performing Grid Search with 3-fold CV...\n",
      "Fitting 3 folds for each of 90 candidates, totalling 270 fits\n",
      "Best parameters: {'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Best CV score: 0.7992\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy:  0.8125\n",
      "  Precision: 0.8125\n",
      "  Recall:    0.8125\n",
      "  F1-Score:  0.8125\n",
      "Training: RandomForest\n",
      "Performing Grid Search with 3-fold CV...\n",
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[40]\u001B[39m\u001B[32m, line 192\u001B[39m\n\u001B[32m    189\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m results, best_model, best_model_name\n\u001B[32m    191\u001B[39m \u001B[38;5;66;03m# Run the function\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m192\u001B[39m results_tuned, best_model_tuned, best_model_name_tuned = \u001B[43mtrain_manual_classifier\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    193\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_train_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    194\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_test_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    195\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_type_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameter_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    196\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m    198\u001B[39m \u001B[38;5;66;03m# Display results summary\u001B[39;00m\n\u001B[32m    199\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m + \u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m*\u001B[32m80\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[40]\u001B[39m\u001B[32m, line 138\u001B[39m, in \u001B[36mtrain_manual_classifier\u001B[39m\u001B[34m(X_train, y_train, X_test, y_test, model_type_list, parameter_list, cv)\u001B[39m\n\u001B[32m    129\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPerforming Grid Search with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcv\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m-fold CV...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    130\u001B[39m grid_search = GridSearchCV(\n\u001B[32m    131\u001B[39m     estimator=model,\n\u001B[32m    132\u001B[39m     param_grid=parameter_list[model_name],\n\u001B[32m   (...)\u001B[39m\u001B[32m    136\u001B[39m     verbose=\u001B[32m1\u001B[39m\n\u001B[32m    137\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m138\u001B[39m \u001B[43mgrid_search\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    139\u001B[39m trained_model = grid_search.best_estimator_  \u001B[38;5;66;03m# the best estimator\u001B[39;00m\n\u001B[32m    140\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mBest parameters: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgrid_search.best_params_\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)  \u001B[38;5;66;03m# the best parameters of the best estimator\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1382\u001B[39m     estimator._validate_params()\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1385\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m     )\n\u001B[32m   1388\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001B[39m, in \u001B[36mBaseSearchCV.fit\u001B[39m\u001B[34m(self, X, y, **params)\u001B[39m\n\u001B[32m   1018\u001B[39m     results = \u001B[38;5;28mself\u001B[39m._format_results(\n\u001B[32m   1019\u001B[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[32m   1020\u001B[39m     )\n\u001B[32m   1022\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[32m-> \u001B[39m\u001B[32m1024\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1026\u001B[39m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[32m   1027\u001B[39m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[32m   1028\u001B[39m first_test_score = all_out[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mtest_scores\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001B[39m, in \u001B[36mGridSearchCV._run_search\u001B[39m\u001B[34m(self, evaluate_candidates)\u001B[39m\n\u001B[32m   1569\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[32m   1570\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1571\u001B[39m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001B[39m, in \u001B[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[39m\u001B[34m(candidate_params, cv, more_results)\u001B[39m\n\u001B[32m    962\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.verbose > \u001B[32m0\u001B[39m:\n\u001B[32m    963\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[32m    964\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[33m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[33m candidates,\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    965\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m fits\u001B[39m\u001B[33m\"\u001B[39m.format(\n\u001B[32m    966\u001B[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001B[32m    967\u001B[39m         )\n\u001B[32m    968\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m970\u001B[39m out = \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    971\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    972\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    973\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    974\u001B[39m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    975\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    976\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    977\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    978\u001B[39m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    979\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    980\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    981\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    982\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    983\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    984\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplitter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    985\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    986\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    988\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) < \u001B[32m1\u001B[39m:\n\u001B[32m    989\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    990\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mNo fits were performed. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    991\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWas the CV iterator empty? \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    992\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWere there no candidates?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    993\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m     72\u001B[39m config = get_config()\n\u001B[32m     73\u001B[39m iterable_with_config = (\n\u001B[32m     74\u001B[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[32m     76\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:2072\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   2066\u001B[39m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[32m   2067\u001B[39m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[32m   2068\u001B[39m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[32m   2069\u001B[39m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[32m   2070\u001B[39m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[32m-> \u001B[39m\u001B[32m2072\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1682\u001B[39m, in \u001B[36mParallel._get_outputs\u001B[39m\u001B[34m(self, iterator, pre_dispatch)\u001B[39m\n\u001B[32m   1679\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[32m   1681\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backend.retrieval_context():\n\u001B[32m-> \u001B[39m\u001B[32m1682\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m._retrieve()\n\u001B[32m   1684\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[32m   1685\u001B[39m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[32m   1686\u001B[39m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[32m   1687\u001B[39m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[32m   1688\u001B[39m     \u001B[38;5;28mself\u001B[39m._exception = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1800\u001B[39m, in \u001B[36mParallel._retrieve\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1789\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_ordered:\n\u001B[32m   1790\u001B[39m     \u001B[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001B[39;00m\n\u001B[32m   1791\u001B[39m     \u001B[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1795\u001B[39m     \u001B[38;5;66;03m# control only have to be done on the amount of time the next\u001B[39;00m\n\u001B[32m   1796\u001B[39m     \u001B[38;5;66;03m# dispatched job is pending.\u001B[39;00m\n\u001B[32m   1797\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m (nb_jobs == \u001B[32m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   1798\u001B[39m         \u001B[38;5;28mself\u001B[39m._jobs[\u001B[32m0\u001B[39m].get_status(timeout=\u001B[38;5;28mself\u001B[39m.timeout) == TASK_PENDING\n\u001B[32m   1799\u001B[39m     ):\n\u001B[32m-> \u001B[39m\u001B[32m1800\u001B[39m         \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1801\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m   1803\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m nb_jobs == \u001B[32m0\u001B[39m:\n\u001B[32m   1804\u001B[39m     \u001B[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001B[39;00m\n\u001B[32m   1805\u001B[39m     \u001B[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1811\u001B[39m     \u001B[38;5;66;03m# timeouts before any other dispatched job has completed and\u001B[39;00m\n\u001B[32m   1812\u001B[39m     \u001B[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adding CPU parameter grids",
   "id": "4f708b1a67d9da6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 2: Make a parameter list with respect to each model type\n",
    "parameter_list = {\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'max_depth': [5, 10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.5, 1.0]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "    }\n",
    "}"
   ],
   "id": "adbe9261dca3e26e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CPU Training Function",
   "id": "486a5d8bfaeb1884"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Commented out: Original manually wriiten CPU only training function\n",
    "def train_manual_classifier(X_train, y_train, X_test, y_test,\n",
    "                            model_type_list, parameter_list,\n",
    "                            cv=3):\n",
    "    results = {}\n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    best_model_name = None\n",
    "\n",
    "    for model_name, model in model_type_list.items():\n",
    "        print(f\"Training: {model_name}\")\n",
    "        try:\n",
    "            if model_name in parameter_list:\n",
    "                print(f\"Performing Grid Search with {cv}-fold CV...\")\n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=model,\n",
    "                    param_grid=parameter_list[model_name],\n",
    "                    cv=cv,\n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=-1,\n",
    "                    verbose=1\n",
    "                )\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                trained_model = grid_search.best_estimator_  # the best estimator\n",
    "                print(f\"Best parameters: {grid_search.best_params_}\")  # the best parameters of the best estimator\n",
    "                print(f\"Best CV score: {grid_search.best_score_:.4f}\")  # the best score from grid search\n",
    "            else:\n",
    "                print(\"Training with default parameters...\")\n",
    "                trained_model = model\n",
    "                trained_model.fit(X_train, y_train)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = trained_model.predict(X_test)\n",
    "\n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "            # Store results\n",
    "            results[model_name] = {\n",
    "                'model': trained_model,\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'predictions': y_pred\n",
    "            }\n",
    "\n",
    "            # Print metrics\n",
    "            print(f\"\\nTest Set Performance:\")\n",
    "            print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall:    {recall:.4f}\")\n",
    "            print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "            # Track best model\n",
    "            if accuracy > best_score:\n",
    "                best_score = accuracy\n",
    "                best_model = trained_model\n",
    "                best_model_name = model_name\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_name}: {str(e)}\")\n",
    "            results[model_name] = {\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "    print(f\"BEST MODEL: {best_model_name}\")\n",
    "    print(f\"BEST ACCURACY: {best_score:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    return results, best_model, best_model_name\n",
    "\n",
    "# Run the function\n",
    "results_tuned, best_model_tuned, best_model_name_tuned = train_manual_classifier(\n",
    "    X_train_scaled, y_train,\n",
    "    X_test_scaled, y_test,\n",
    "    model_type_list, parameter_list,\n",
    ")\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "results_df = pd.DataFrame({\n",
    "    model_name: {\n",
    "        'Accuracy': metrics.get('accuracy', 0),\n",
    "        'Precision': metrics.get('precision', 0),\n",
    "        'Recall': metrics.get('recall', 0),\n",
    "        'F1-Score': metrics.get('f1_score', 0)\n",
    "    }\n",
    "    for model_name, metrics in results_tuned.items() if 'accuracy' in metrics\n",
    "}).T.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "# Print detailed classification report for best model\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DETAILED CLASSIFICATION REPORT - {best_model_name_tuned}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(classification_report(y_test, results_tuned[best_model_name_tuned]['predictions'],\n",
    "                          target_names=label_encoder.classes_))"
   ],
   "id": "64973348a580f732"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adding GPU models",
   "id": "8715291bea1c4149"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T20:02:38.711164200Z",
     "start_time": "2025-11-06T19:34:40.029025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "# Import GPU-accelerated libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "    print(\"✓ XGBoost available\")\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"✗ XGBoost not available - install with: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "    print(\"✓ LightGBM available\")\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"✗ LightGBM not available - install with: pip install lightgbm\")\n",
    "\n",
    "try:\n",
    "    from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "    from cuml.linear_model import LogisticRegression as cuLR\n",
    "    from cuml.svm import SVC as cuSVC\n",
    "    from cuml.neighbors import KNeighborsClassifier as cuKNN\n",
    "    import cupy as cp\n",
    "    CUML_AVAILABLE = True\n",
    "    print(\"✓ CuML (RAPIDS) available - GPU acceleration enabled!\")\n",
    "except ImportError:\n",
    "    CUML_AVAILABLE = False\n",
    "    print(\"ℹ CuML not available - using CPU fallback\")\n",
    "    print(\"  XGBoost and LightGBM will still use GPU acceleration!\")\n",
    "    print(\"  CuML is optional - only install if you need additional GPU models\")\n",
    "    print(\"\")\n",
    "    print(\"  Note: CuML on Windows can be challenging to install. Weird I struggled with AI Agent too\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Step 1: Make GPU-accelerated model type list\n",
    "model_type_list_gpu = {}\n",
    "\n",
    "# Add GPU-accelerated models if available\n",
    "if XGBOOST_AVAILABLE:\n",
    "    print(\"Adding XGBoost GPU models...\")\n",
    "    model_type_list_gpu['XGBoost_GPU'] = xgb.XGBClassifier(\n",
    "        device='cuda',  # GPU acceleration\n",
    "        tree_method='hist',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model_type_list_gpu['XGBoost_CPU'] = xgb.XGBClassifier(\n",
    "        device='cpu',\n",
    "        tree_method='hist',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    print(\"Adding LightGBM GPU models...\")\n",
    "    model_type_list_gpu['LightGBM_GPU'] = lgb.LGBMClassifier(\n",
    "        device='gpu',  # GPU acceleration\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    model_type_list_gpu['LightGBM_CPU'] = lgb.LGBMClassifier(\n",
    "        device='cpu',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "if CUML_AVAILABLE:\n",
    "    print(\"Adding CuML GPU models...\")\n",
    "    # CuML models (RAPIDS) - native GPU support\n",
    "    model_type_list_gpu['RandomForest_GPU'] = cuRF(n_estimators=100, random_state=42)\n",
    "    model_type_list_gpu['LogisticRegression_GPU'] = cuLR(random_state=42, max_iter=1000)\n",
    "    model_type_list_gpu['SVM_GPU'] = cuSVC(random_state=42)\n",
    "    model_type_list_gpu['KNN_GPU'] = cuKNN(n_neighbors=5)\n",
    "else:\n",
    "    # Fallback to CPU models from sklearn\n",
    "    print(\"Adding CPU fallback models...\")\n",
    "    model_type_list_gpu['RandomForest_CPU'] = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=100)\n",
    "    model_type_list_gpu['LogisticRegression_CPU'] = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    model_type_list_gpu['SVM_CPU'] = SVC(random_state=42)\n",
    "    model_type_list_gpu['KNN_CPU'] = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "# CPU models for comparison\n",
    "model_type_list_gpu['DecisionTree'] = DecisionTreeClassifier(random_state=42)\n",
    "model_type_list_gpu['GradientBoosting'] = GradientBoostingClassifier(random_state=42)\n",
    "model_type_list_gpu['AdaBoost'] = AdaBoostClassifier(random_state=42)\n",
    "model_type_list_gpu['GaussianNB'] = GaussianNB()\n",
    "\n",
    "print(f\"Total models to train(GPU Focused): {len(model_type_list_gpu)}\")\n",
    "print(f\"Models: {list(model_type_list_gpu.keys())}\")\n"
   ],
   "id": "ee0f219a84d91bf2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ XGBoost available\n",
      "✓ LightGBM available\n",
      "ℹ CuML not available - using CPU fallback (THIS IS NORMAL)\n",
      "  XGBoost and LightGBM will still use GPU acceleration!\n",
      "  CuML is optional - only install if you need additional GPU models\n",
      "\n",
      "  Note: CuML on Windows can be challenging to install.\n",
      "  Your GPU training will work great with XGBoost and LightGBM!\n",
      "\n",
      "================================================================================\n",
      "Adding XGBoost GPU models...\n",
      "Adding LightGBM GPU models...\n",
      "Adding CPU fallback models...\n",
      "\n",
      "✓ Total models to train: 12\n",
      "Models: ['XGBoost_GPU', 'XGBoost_CPU', 'LightGBM_GPU', 'LightGBM_CPU', 'RandomForest_CPU', 'LogisticRegression_CPU', 'SVM_CPU', 'KNN_CPU', 'DecisionTree', 'GradientBoosting', 'AdaBoost', 'GaussianNB']\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adding GPU parameter grids",
   "id": "cc88b7ae33a2f9a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T19:34:44.522771Z",
     "start_time": "2025-11-06T19:34:44.505865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Make parameter list for GPU models\n",
    "parameter_list_gpu = {}\n",
    "\n",
    "# XGBoost parameters (works for both GPU and CPU)\n",
    "if XGBOOST_AVAILABLE:\n",
    "    parameter_list_gpu['XGBoost_GPU'] = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    parameter_list_gpu['XGBoost_CPU'] = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "\n",
    "# LightGBM parameters (works for both GPU and CPU)\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    parameter_list_gpu['LightGBM_GPU'] = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7, -1],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'num_leaves': [31, 50, 100],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "    parameter_list_gpu['LightGBM_CPU'] = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7, -1],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'num_leaves': [31, 50, 100],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "\n",
    "# CuML/sklearn RandomForest parameters\n",
    "if CUML_AVAILABLE:\n",
    "    parameter_list_gpu['RandomForest_GPU'] = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, 30],\n",
    "        'max_features': [0.5, 0.8, 1.0]\n",
    "    }\n",
    "else:\n",
    "    parameter_list_gpu['RandomForest_CPU'] = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "\n",
    "# Logistic Regression parameters\n",
    "if CUML_AVAILABLE:\n",
    "    parameter_list_gpu['LogisticRegression_GPU'] = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'max_iter': [1000, 2000]\n",
    "    }\n",
    "else:\n",
    "    parameter_list_gpu['LogisticRegression_CPU'] = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    }\n",
    "\n",
    "# Other model parameters\n",
    "parameter_list_gpu['DecisionTree'] = {\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "parameter_list_gpu['GradientBoosting'] = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "parameter_list_gpu['AdaBoost'] = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "parameter_list_gpu['GaussianNB'] = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "}\n",
    "\n",
    "print(f\"✓ Parameter grids configured for {len(parameter_list_gpu)} models\")\n"
   ],
   "id": "478dd820dcedce3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Parameter grids configured for 10 models\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GPU Training Function",
   "id": "e1c7b5ab1ff57ec5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T19:34:49.334618Z",
     "start_time": "2025-11-06T19:34:49.322200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3: GPU-accelerated training function\n",
    "def train_gpu_classifier(X_train, y_train, X_test, y_test,\n",
    "                         model_type_list, parameter_list,\n",
    "                         use_grid_search=False, cv=3):\n",
    "    \"\"\"\n",
    "    Train multiple classifiers with GPU acceleration and optional hyperparameter tuning.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : array-like\n",
    "        Training features\n",
    "    y_train : array-like\n",
    "        Training labels\n",
    "    X_test : array-like\n",
    "        Test features\n",
    "    y_test : array-like\n",
    "        Test labels\n",
    "    model_type_list : dict\n",
    "        Dictionary of model names and instances\n",
    "    parameter_list : dict\n",
    "        Dictionary of hyperparameter grids for each model\n",
    "    use_grid_search : bool\n",
    "        Whether to perform grid search for hyperparameter tuning\n",
    "    cv : int\n",
    "        Number of cross-validation folds for grid search\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary containing model performance metrics\n",
    "    best_model : object\n",
    "        The best performing model\n",
    "    best_model_name : str\n",
    "        Name of the best performing model\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    best_model_name = None\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"TRAINING MODELS (GPU-ACCELERATED)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for model_name, model in model_type_list.items():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training: {model_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Convert data for CuML models (they need cupy arrays)\n",
    "            if CUML_AVAILABLE and '_GPU' in model_name and 'XGBoost' not in model_name and 'LightGBM' not in model_name:\n",
    "                X_train_gpu = cp.array(X_train)\n",
    "                y_train_gpu = cp.array(y_train)\n",
    "                X_test_gpu = cp.array(X_test)\n",
    "                y_test_gpu = cp.array(y_test)\n",
    "            else:\n",
    "                X_train_gpu = X_train\n",
    "                y_train_gpu = y_train\n",
    "                X_test_gpu = X_test\n",
    "                y_test_gpu = y_test\n",
    "\n",
    "            if use_grid_search and model_name in parameter_list:\n",
    "                print(f\"Performing Grid Search with {cv}-fold CV...\")\n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=model,\n",
    "                    param_grid=parameter_list[model_name],\n",
    "                    cv=cv,\n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=-1 if 'GPU' not in model_name else 1,\n",
    "                    verbose=1\n",
    "                )\n",
    "                grid_search.fit(X_train_gpu, y_train_gpu)\n",
    "                trained_model = grid_search.best_estimator_\n",
    "                print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "                print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "            else:\n",
    "                print(\"Training with default parameters...\")\n",
    "                trained_model = model\n",
    "                trained_model.fit(X_train_gpu, y_train_gpu)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = trained_model.predict(X_test_gpu)\n",
    "\n",
    "            # Convert back from GPU if needed\n",
    "            if CUML_AVAILABLE and '_GPU' in model_name and 'XGBoost' not in model_name and 'LightGBM' not in model_name:\n",
    "                y_pred = cp.asnumpy(y_pred)\n",
    "                y_test_cpu = cp.asnumpy(y_test_gpu)\n",
    "            else:\n",
    "                y_test_cpu = y_test\n",
    "\n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test_cpu, y_pred)\n",
    "            precision = precision_score(y_test_cpu, y_pred, average='weighted', zero_division=0)\n",
    "            recall = recall_score(y_test_cpu, y_pred, average='weighted', zero_division=0)\n",
    "            f1 = f1_score(y_test_cpu, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            # Store results\n",
    "            results[model_name] = {\n",
    "                'model': trained_model,\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'predictions': y_pred,\n",
    "                'training_time': training_time\n",
    "            }\n",
    "\n",
    "            # Print metrics\n",
    "            print(f\"\\nTest Set Performance:\")\n",
    "            print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall:    {recall:.4f}\")\n",
    "            print(f\"  F1-Score:  {f1:.4f}\")\n",
    "            print(f\"  Time:      {training_time:.2f}s {'🚀 (GPU)' if '_GPU' in model_name else '(CPU)'}\")\n",
    "\n",
    "            # Track best model\n",
    "            if accuracy > best_score:\n",
    "                best_score = accuracy\n",
    "                best_model = trained_model\n",
    "                best_model_name = model_name\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error training {model_name}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            results[model_name] = {\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🏆 BEST MODEL: {best_model_name}\")\n",
    "    print(f\"🎯 BEST ACCURACY: {best_score:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    return results, best_model, best_model_name\n"
   ],
   "id": "3bc526eca0dd411",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T19:35:05.712782Z",
     "start_time": "2025-11-06T19:34:54.276708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 4: Train models with GPU acceleration\n",
    "print(\"🚀 Starting GPU-accelerated training...\")\n",
    "print(\"\\n💡 TIP: Open another terminal and run 'nvidia-smi -l 1' to monitor GPU usage\\n\")\n",
    "\n",
    "# Train without grid search first (faster)\n",
    "results_gpu, best_model_gpu, best_model_name_gpu = train_gpu_classifier(\n",
    "    X_train_scaled, y_train,\n",
    "    X_test_scaled, y_test,\n",
    "    model_type_list_gpu, parameter_list_gpu,\n",
    "    use_grid_search=False,  # Set to True for hyperparameter tuning (slower but better)\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 RESULTS SUMMARY (GPU vs CPU)\")\n",
    "print(\"=\"*80)\n",
    "results_df_gpu = pd.DataFrame({\n",
    "    model_name: {\n",
    "        'Accuracy': metrics.get('accuracy', 0),\n",
    "        'Precision': metrics.get('precision', 0),\n",
    "        'Recall': metrics.get('recall', 0),\n",
    "        'F1-Score': metrics.get('f1_score', 0),\n",
    "        'Time (s)': metrics.get('training_time', 0)\n",
    "    }\n",
    "    for model_name, metrics in results_gpu.items() if 'accuracy' in metrics\n",
    "}).T.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(results_df_gpu)\n",
    "\n",
    "# Print detailed classification report for best model\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"📋 DETAILED CLASSIFICATION REPORT - {best_model_name_gpu}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(classification_report(y_test, results_gpu[best_model_name_gpu]['predictions'],\n",
    "                          target_names=label_encoder.classes_))\n",
    "\n",
    "# Save the best model\n",
    "model_filename = f'url_classifier_{best_model_name_gpu}.pkl'\n",
    "joblib.dump(best_model_gpu, model_filename)\n",
    "print(f\"\\n✅ Best model saved as '{model_filename}'\")\n",
    "\n",
    "# Compare GPU vs CPU performance if both are available\n",
    "if XGBOOST_AVAILABLE or LIGHTGBM_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"⚡ GPU SPEEDUP COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    for model_name in results_gpu.keys():\n",
    "        if '_GPU' in model_name and 'accuracy' in results_gpu[model_name]:\n",
    "            cpu_model_name = model_name.replace('_GPU', '_CPU')\n",
    "            if cpu_model_name in results_gpu and 'training_time' in results_gpu[cpu_model_name]:\n",
    "                gpu_time = results_gpu[model_name]['training_time']\n",
    "                cpu_time = results_gpu[cpu_model_name]['training_time']\n",
    "                speedup = cpu_time / gpu_time if gpu_time > 0 else 0\n",
    "                print(f\"{model_name:25} GPU: {gpu_time:6.2f}s | CPU: {cpu_time:6.2f}s | Speedup: {speedup:.2f}x\")\n"
   ],
   "id": "1e62805f006cc0bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting GPU-accelerated training...\n",
      "\n",
      "💡 TIP: Open another terminal and run 'nvidia-smi -l 1' to monitor GPU usage\n",
      "\n",
      "================================================================================\n",
      "TRAINING MODELS (GPU-ACCELERATED)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Training: XGBoost_GPU\n",
      "================================================================================\n",
      "Training with default parameters...\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy:  0.8555\n",
      "  Precision: 0.8556\n",
      "  Recall:    0.8555\n",
      "  F1-Score:  0.8555\n",
      "  Time:      0.56s 🚀 (GPU)\n",
      "\n",
      "================================================================================\n",
      "Training: XGBoost_CPU\n",
      "================================================================================\n",
      "Training with default parameters...\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy:  0.8558\n",
      "  Precision: 0.8559\n",
      "  Recall:    0.8558\n",
      "  F1-Score:  0.8558\n",
      "  Time:      0.26s (CPU)\n",
      "\n",
      "================================================================================\n",
      "Training: LightGBM_GPU\n",
      "================================================================================\n",
      "Training with default parameters...\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy:  0.8481\n",
      "  Precision: 0.8482\n",
      "  Recall:    0.8481\n",
      "  F1-Score:  0.8481\n",
      "  Time:      3.74s 🚀 (GPU)\n",
      "\n",
      "================================================================================\n",
      "Training: LightGBM_CPU\n",
      "================================================================================\n",
      "Training with default parameters...\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy:  0.8481\n",
      "  Precision: 0.8482\n",
      "  Recall:    0.8481\n",
      "  F1-Score:  0.8481\n",
      "  Time:      0.30s (CPU)\n",
      "\n",
      "================================================================================\n",
      "Training: RandomForest_CPU\n",
      "================================================================================\n",
      "Training with default parameters...\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy:  0.8579\n",
      "  Precision: 0.8586\n",
      "  Recall:    0.8579\n",
      "  F1-Score:  0.8579\n",
      "  Time:      0.71s (CPU)\n",
      "\n",
      "================================================================================\n",
      "Training: LogisticRegression_CPU\n",
      "================================================================================\n",
      "Training with default parameters...\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy:  0.7043\n",
      "  Precision: 0.7105\n",
      "  Recall:    0.7043\n",
      "  F1-Score:  0.7024\n",
      "  Time:      0.03s (CPU)\n",
      "\n",
      "================================================================================\n",
      "Training: SVM_CPU\n",
      "================================================================================\n",
      "Training with default parameters...\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy:  0.7981\n",
      "  Precision: 0.7981\n",
      "  Recall:    0.7981\n",
      "  F1-Score:  0.7981\n",
      "  Time:      3.75s (CPU)\n",
      "\n",
      "================================================================================\n",
      "Training: KNN_CPU\n",
      "================================================================================\n",
      "Training with default parameters...\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy:  0.8111\n",
      "  Precision: 0.8111\n",
      "  Recall:    0.8111\n",
      "  F1-Score:  0.8111\n",
      "  Time:      0.33s (CPU)\n",
      "\n",
      "================================================================================\n",
      "Training: DecisionTree\n",
      "================================================================================\n",
      "Training with default parameters...\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy:  0.8041\n",
      "  Precision: 0.8041\n",
      "  Recall:    0.8041\n",
      "  F1-Score:  0.8041\n",
      "  Time:      0.06s (CPU)\n",
      "\n",
      "================================================================================\n",
      "Training: GradientBoosting\n",
      "================================================================================\n",
      "Training with default parameters...\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy:  0.8240\n",
      "  Precision: 0.8240\n",
      "  Recall:    0.8240\n",
      "  F1-Score:  0.8240\n",
      "  Time:      1.22s (CPU)\n",
      "\n",
      "================================================================================\n",
      "Training: AdaBoost\n",
      "================================================================================\n",
      "Training with default parameters...\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy:  0.8114\n",
      "  Precision: 0.8114\n",
      "  Recall:    0.8114\n",
      "  F1-Score:  0.8114\n",
      "  Time:      0.34s (CPU)\n",
      "\n",
      "================================================================================\n",
      "Training: GaussianNB\n",
      "================================================================================\n",
      "Training with default parameters...\n",
      "\n",
      "Test Set Performance:\n",
      "  Accuracy:  0.6707\n",
      "  Precision: 0.7239\n",
      "  Recall:    0.6707\n",
      "  F1-Score:  0.6506\n",
      "  Time:      0.01s (CPU)\n",
      "\n",
      "================================================================================\n",
      "🏆 BEST MODEL: RandomForest_CPU\n",
      "🎯 BEST ACCURACY: 0.8579\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "📊 RESULTS SUMMARY (GPU vs CPU)\n",
      "================================================================================\n",
      "                        Accuracy  Precision    Recall  F1-Score  Time (s)\n",
      "RandomForest_CPU        0.857943   0.858604  0.857943  0.857895  0.708823\n",
      "XGBoost_CPU             0.855843   0.855850  0.855843  0.855840  0.262250\n",
      "XGBoost_GPU             0.855493   0.855560  0.855493  0.855480  0.559628\n",
      "LightGBM_GPU            0.848146   0.848164  0.848146  0.848140  3.743090\n",
      "LightGBM_CPU            0.848146   0.848164  0.848146  0.848140  0.299660\n",
      "GradientBoosting        0.824003   0.824004  0.824003  0.824001  1.218437\n",
      "AdaBoost                0.811407   0.811432  0.811407  0.811407  0.339104\n",
      "KNN_CPU                 0.811057   0.811057  0.811057  0.811057  0.327537\n",
      "DecisionTree            0.804059   0.804132  0.804059  0.804056  0.055550\n",
      "SVM_CPU                 0.798111   0.798114  0.798111  0.798111  3.752965\n",
      "LogisticRegression_CPU  0.704339   0.710458  0.704339  0.702380  0.025327\n",
      "GaussianNB              0.670749   0.723948  0.670749  0.650647  0.013633\n",
      "\n",
      "================================================================================\n",
      "📋 DETAILED CLASSIFICATION REPORT - RandomForest_CPU\n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  legitimate       0.84      0.88      0.86      1422\n",
      "    phishing       0.87      0.84      0.86      1436\n",
      "\n",
      "    accuracy                           0.86      2858\n",
      "   macro avg       0.86      0.86      0.86      2858\n",
      "weighted avg       0.86      0.86      0.86      2858\n",
      "\n",
      "\n",
      "✅ Best model saved as 'url_classifier_RandomForest_CPU.pkl'\n",
      "\n",
      "================================================================================\n",
      "⚡ GPU SPEEDUP COMPARISON\n",
      "================================================================================\n",
      "XGBoost_GPU               GPU:   0.56s | CPU:   0.26s | Speedup: 0.47x\n",
      "LightGBM_GPU              GPU:   3.74s | CPU:   0.30s | Speedup: 0.08x\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T20:19:37.813089Z",
     "start_time": "2025-11-06T20:17:46.909314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Optional: Train with Grid Search for hyperparameter tuning (uncomment to use)\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"🔧 TRAINING WITH HYPERPARAMETER TUNING (This will take longer)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_gpu_tuned, best_model_gpu_tuned, best_model_name_gpu_tuned = train_gpu_classifier(\n",
    "    X_train_scaled, y_train,\n",
    "    X_test_scaled, y_test,\n",
    "    model_type_list_gpu, parameter_list_gpu,\n",
    "    use_grid_search=True,  # Enable grid search\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "print(\"\\n📊 TUNED RESULTS:\")\n",
    "results_df_tuned = pd.DataFrame({\n",
    "    model_name: {\n",
    "        'Accuracy': metrics.get('accuracy', 0),\n",
    "        'Precision': metrics.get('precision', 0),\n",
    "        'Recall': metrics.get('recall', 0),\n",
    "        'F1-Score': metrics.get('f1_score', 0),\n",
    "        'Time (s)': metrics.get('training_time', 0)\n",
    "    }\n",
    "    for model_name, metrics in results_gpu_tuned.items() if 'accuracy' in metrics\n",
    "}).T.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(results_df_tuned)"
   ],
   "id": "ee80bb6651f73ee4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "🔧 TRAINING WITH HYPERPARAMETER TUNING (This will take longer)\n",
      "================================================================================\n",
      "================================================================================\n",
      "TRAINING MODELS (GPU-ACCELERATED)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Training: XGBoost_GPU\n",
      "================================================================================\n",
      "Performing Grid Search with 5-fold CV...\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[46]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m🔧 TRAINING WITH HYPERPARAMETER TUNING (This will take longer)\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m*\u001B[32m80\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m results_gpu_tuned, best_model_gpu_tuned, best_model_name_gpu_tuned = \u001B[43mtrain_gpu_classifier\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_train_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_test_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_type_list_gpu\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameter_list_gpu\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_grid_search\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Enable grid search\u001B[39;49;00m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcv\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\n\u001B[32m     12\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m📊 TUNED RESULTS:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     15\u001B[39m results_df_tuned = pd.DataFrame({\n\u001B[32m     16\u001B[39m     model_name: {\n\u001B[32m     17\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mAccuracy\u001B[39m\u001B[33m'\u001B[39m: metrics.get(\u001B[33m'\u001B[39m\u001B[33maccuracy\u001B[39m\u001B[33m'\u001B[39m, \u001B[32m0\u001B[39m),\n\u001B[32m   (...)\u001B[39m\u001B[32m     23\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m model_name, metrics \u001B[38;5;129;01min\u001B[39;00m results_gpu_tuned.items() \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33maccuracy\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m metrics\n\u001B[32m     24\u001B[39m }).T.sort_values(\u001B[33m'\u001B[39m\u001B[33mAccuracy\u001B[39m\u001B[33m'\u001B[39m, ascending=\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[43]\u001B[39m\u001B[32m, line 75\u001B[39m, in \u001B[36mtrain_gpu_classifier\u001B[39m\u001B[34m(X_train, y_train, X_test, y_test, model_type_list, parameter_list, use_grid_search, cv)\u001B[39m\n\u001B[32m     66\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPerforming Grid Search with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcv\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m-fold CV...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     67\u001B[39m grid_search = GridSearchCV(\n\u001B[32m     68\u001B[39m     estimator=model,\n\u001B[32m     69\u001B[39m     param_grid=parameter_list[model_name],\n\u001B[32m   (...)\u001B[39m\u001B[32m     73\u001B[39m     verbose=\u001B[32m1\u001B[39m\n\u001B[32m     74\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m75\u001B[39m \u001B[43mgrid_search\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_gpu\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_gpu\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     76\u001B[39m trained_model = grid_search.best_estimator_\n\u001B[32m     77\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mBest parameters: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgrid_search.best_params_\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1382\u001B[39m     estimator._validate_params()\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1385\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m     )\n\u001B[32m   1388\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001B[39m, in \u001B[36mBaseSearchCV.fit\u001B[39m\u001B[34m(self, X, y, **params)\u001B[39m\n\u001B[32m   1018\u001B[39m     results = \u001B[38;5;28mself\u001B[39m._format_results(\n\u001B[32m   1019\u001B[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[32m   1020\u001B[39m     )\n\u001B[32m   1022\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[32m-> \u001B[39m\u001B[32m1024\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1026\u001B[39m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[32m   1027\u001B[39m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[32m   1028\u001B[39m first_test_score = all_out[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mtest_scores\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001B[39m, in \u001B[36mGridSearchCV._run_search\u001B[39m\u001B[34m(self, evaluate_candidates)\u001B[39m\n\u001B[32m   1569\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[32m   1570\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1571\u001B[39m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001B[39m, in \u001B[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[39m\u001B[34m(candidate_params, cv, more_results)\u001B[39m\n\u001B[32m    962\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.verbose > \u001B[32m0\u001B[39m:\n\u001B[32m    963\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[32m    964\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[33m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[33m candidates,\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    965\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m fits\u001B[39m\u001B[33m\"\u001B[39m.format(\n\u001B[32m    966\u001B[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001B[32m    967\u001B[39m         )\n\u001B[32m    968\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m970\u001B[39m out = \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    971\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    972\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    973\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    974\u001B[39m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    975\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    976\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    977\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    978\u001B[39m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    979\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    980\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    981\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    982\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    983\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    984\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplitter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    985\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    986\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    988\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) < \u001B[32m1\u001B[39m:\n\u001B[32m    989\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    990\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mNo fits were performed. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    991\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWas the CV iterator empty? \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    992\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWere there no candidates?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    993\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m     72\u001B[39m config = get_config()\n\u001B[32m     73\u001B[39m iterable_with_config = (\n\u001B[32m     74\u001B[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[32m     76\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1986\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   1984\u001B[39m     output = \u001B[38;5;28mself\u001B[39m._get_sequential_output(iterable)\n\u001B[32m   1985\u001B[39m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[32m-> \u001B[39m\u001B[32m1986\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1988\u001B[39m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[32m   1989\u001B[39m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[32m   1990\u001B[39m \u001B[38;5;66;03m# reused, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[32m   1991\u001B[39m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[32m   1992\u001B[39m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[32m   1993\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._lock:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1914\u001B[39m, in \u001B[36mParallel._get_sequential_output\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   1912\u001B[39m \u001B[38;5;28mself\u001B[39m.n_dispatched_batches += \u001B[32m1\u001B[39m\n\u001B[32m   1913\u001B[39m \u001B[38;5;28mself\u001B[39m.n_dispatched_tasks += \u001B[32m1\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1914\u001B[39m res = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1915\u001B[39m \u001B[38;5;28mself\u001B[39m.n_completed_tasks += \u001B[32m1\u001B[39m\n\u001B[32m   1916\u001B[39m \u001B[38;5;28mself\u001B[39m.print_progress()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001B[39m, in \u001B[36m_FuncWrapper.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    137\u001B[39m     config = {}\n\u001B[32m    138\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(**config):\n\u001B[32m--> \u001B[39m\u001B[32m139\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:866\u001B[39m, in \u001B[36m_fit_and_score\u001B[39m\u001B[34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[39m\n\u001B[32m    864\u001B[39m         estimator.fit(X_train, **fit_params)\n\u001B[32m    865\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m866\u001B[39m         \u001B[43mestimator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Note fit time as time until error\u001B[39;00m\n\u001B[32m    870\u001B[39m     fit_time = time.time() - start_time\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\xgboost\\core.py:774\u001B[39m, in \u001B[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    772\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig.parameters, args):\n\u001B[32m    773\u001B[39m     kwargs[k] = arg\n\u001B[32m--> \u001B[39m\u001B[32m774\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1803\u001B[39m, in \u001B[36mXGBClassifier.fit\u001B[39m\u001B[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001B[39m\n\u001B[32m   1783\u001B[39m evals_result: EvalsLog = {}\n\u001B[32m   1784\u001B[39m train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001B[32m   1785\u001B[39m     missing=\u001B[38;5;28mself\u001B[39m.missing,\n\u001B[32m   1786\u001B[39m     X=X,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1800\u001B[39m     feature_types=feature_types,\n\u001B[32m   1801\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1803\u001B[39m \u001B[38;5;28mself\u001B[39m._Booster = \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1804\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1805\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_dmatrix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1806\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_num_boosting_rounds\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1807\u001B[39m \u001B[43m    \u001B[49m\u001B[43mevals\u001B[49m\u001B[43m=\u001B[49m\u001B[43mevals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1808\u001B[39m \u001B[43m    \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1809\u001B[39m \u001B[43m    \u001B[49m\u001B[43mevals_result\u001B[49m\u001B[43m=\u001B[49m\u001B[43mevals_result\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1810\u001B[39m \u001B[43m    \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m=\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1811\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcustom_metric\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1812\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1813\u001B[39m \u001B[43m    \u001B[49m\u001B[43mxgb_model\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1814\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1815\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1817\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m.objective):\n\u001B[32m   1818\u001B[39m     \u001B[38;5;28mself\u001B[39m.objective = params[\u001B[33m\"\u001B[39m\u001B[33mobjective\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\xgboost\\core.py:774\u001B[39m, in \u001B[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    772\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig.parameters, args):\n\u001B[32m    773\u001B[39m     kwargs[k] = arg\n\u001B[32m--> \u001B[39m\u001B[32m774\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\xgboost\\training.py:199\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001B[39m\n\u001B[32m    197\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001B[32m    198\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m199\u001B[39m \u001B[43mbst\u001B[49m\u001B[43m.\u001B[49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miteration\u001B[49m\u001B[43m=\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfobj\u001B[49m\u001B[43m=\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    200\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001B[32m    201\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\Lib\\site-packages\\xgboost\\core.py:2434\u001B[39m, in \u001B[36mBooster.update\u001B[39m\u001B[34m(self, dtrain, iteration, fobj)\u001B[39m\n\u001B[32m   2430\u001B[39m \u001B[38;5;28mself\u001B[39m._assign_dmatrix_features(dtrain)\n\u001B[32m   2432\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m fobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   2433\u001B[39m     _check_call(\n\u001B[32m-> \u001B[39m\u001B[32m2434\u001B[39m         \u001B[43m_LIB\u001B[49m\u001B[43m.\u001B[49m\u001B[43mXGBoosterUpdateOneIter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2435\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctypes\u001B[49m\u001B[43m.\u001B[49m\u001B[43mc_int\u001B[49m\u001B[43m(\u001B[49m\u001B[43miteration\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle\u001B[49m\n\u001B[32m   2436\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2437\u001B[39m     )\n\u001B[32m   2438\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2439\u001B[39m     pred = \u001B[38;5;28mself\u001B[39m.predict(dtrain, output_margin=\u001B[38;5;28;01mTrue\u001B[39;00m, training=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "827aa7ebc58b62fd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
